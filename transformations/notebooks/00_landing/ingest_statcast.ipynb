{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68cd20d7-3000-445b-974d-d3100e760a14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pybaseball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55d065af-192b-4de7-9a90-c7730e27016f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pybaseball import statcast\n",
    "import os\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29fab65d-6687-4d99-a499-9f87d4749fca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "root_path = \"/Volumes/mlb/00_landing/data_sources/statcast\"\n",
    "\n",
    "def get_latest_date_in_volumes(path):\n",
    "    \"\"\"Return the latest date stored in the volumns \"\"\"\n",
    "    try:\n",
    "        years = [f.name.replace('y=', '').replace('/', '') \n",
    "                 for f in dbutils.fs.ls(path) \n",
    "                 if f.name.startswith('y=')]\n",
    "        if not years: return datetime(2024, 3, 28)\n",
    "        latest_y = max(years)\n",
    "        \n",
    "        months = [f.name.replace('m=', '').replace('/', '') \n",
    "                  for f in dbutils.fs.ls(f\"{path}/y={latest_y}\") \n",
    "                  if f.name.startswith('m=')]\n",
    "        latest_m = max(months)\n",
    "        \n",
    "        days = [f.name.replace('d=', '').replace('/', '') \n",
    "                for f in dbutils.fs.ls(f\"{path}/y={latest_y}/m={latest_m}\") \n",
    "                if f.name.startswith('d=')]\n",
    "        latest_d = max(days)\n",
    "        \n",
    "        return datetime.strptime(f\"{latest_y}-{latest_m}-{latest_d}\", '%Y-%m-%d')\n",
    "    except Exception as e:\n",
    "        print(f\"Info: Could not find existing data, starting from default. Details: {e}\")\n",
    "        return datetime(2024, 3, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8219583c-d877-4cc9-8f91-a0439f9e7537",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "latest_stored_date = get_latest_date_in_volumes(root_path)\n",
    "start_date = latest_stored_date + timedelta(days=1)\n",
    "yesterday = datetime.strptime(\"2024-07-10\", '%Y-%m-%d')  # DEBUG\n",
    "# yesterday = datetime.now() - timedelta(days=1)\n",
    "\n",
    "current_date = start_date\n",
    "\n",
    "any_new_data = False\n",
    "processed_dates = []\n",
    "\n",
    "while current_date <= yesterday:\n",
    "    target_date_str = current_date.strftime('%Y-%m-%d')\n",
    "    y, m, d = current_date.strftime('%Y'), current_date.strftime('%m'), current_date.strftime('%d')\n",
    "    dir_path = f\"{root_path}/y={y}/m={m}/d={d}\"\n",
    "    success_file = f\"{dir_path}/_SUCCESS\"\n",
    "\n",
    "    try:\n",
    "        # File exists\n",
    "        dbutils.fs.ls(success_file)\n",
    "    except:\n",
    "        # File does not exists\n",
    "        try:\n",
    "            stat_df = statcast(start_dt=target_date_str, end_dt=target_date_str)\n",
    "            \n",
    "            if stat_df is not None and not stat_df.empty:\n",
    "                df = spark.createDataFrame(stat_df)\n",
    "                \n",
    "                (df.withColumn(\"y\", F.lit(y))\n",
    "                   .withColumn(\"m\", F.lit(m))\n",
    "                   .withColumn(\"d\", F.lit(d))\n",
    "                   .write.format(\"parquet\").mode(\"overwrite\").save(dir_path))\n",
    "        \n",
    "                any_new_data = True\n",
    "                processed_dates.append(target_date_str)\n",
    "                # dbutils.jobs.taskValues.set(key=\"continue_downstream\", value=\"yes\")\n",
    "            else:   \n",
    "                dbutils.fs.put(success_file, \"no data\", overwrite=True)\n",
    "                # dbutils.jobs.taskValues.set(key=\"continue_downstream\", value=\"no\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch {target_date_str}: {e}\")\n",
    "            # dbutils.jobs.taskValues.set(key=\"continue_downstream\", value=\"no\")\n",
    "            raise e\n",
    "            \n",
    "    current_date += timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edffec16-e3d3-4e29-9f4f-d3ddf0d55c1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if any_new_data:\n",
    "    dbutils.jobs.taskValues.set(key=\"continue_downstream\", value=\"yes\")\n",
    "    dbutils.jobs.taskValues.set(key=\"start_date\", value=min(processed_dates))\n",
    "    dbutils.jobs.taskValues.set(key=\"end_date\", value=max(processed_dates))\n",
    "else:\n",
    "    dbutils.jobs.taskValues.set(key=\"continue_downstream\", value=\"no\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ingest_statcast",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
